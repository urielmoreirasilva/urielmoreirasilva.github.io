# T√≥pico 18 ‚Äì Regress√£o Linear [<img src="images/colag_logo.svg" style="float: right; vertical-align: middle; width: 42px; height: 42px;">](https://colab.research.google.com/github/urielmoreirasilva/urielmoreirasilva.github.io/blob/main/aulas/T%C3%B3pico%2018/18%20%E2%80%93%20Regressao.ipynb) [<img src="images/github_logo.svg" style="float: right; margin-right: 12px; vertical-align: middle; width: 36px; height: 36px;">](https://github.com/urielmoreirasilva/urielmoreirasilva.github.io/blob/main/aulas/T%C3%B3pico%2018/18%20%E2%80%93%20Regressao.ipynb)

Finalmente, ap√≥s explorarmos o conceito de correla√ß√£o, chegamos ao √∫ltimo t√≥pico desse curso: o de regress√£o linear!

### Resultados Esperados

1. Definir conceitualmente e formalmente o modelo de regress√£o linear.
1. Aprender como realizar previs√µes atrav√©s de uma regress√£o, e a verificar a qualidade dessas previs√µes na pr√°tica.
1. Discutir o papel dos outliers e das unidades de medida nas nossas previs√µes, e estabelecer uma conex√£o importante entre regress√£o e correla√ß√£o.

### Refer√™ncias
- [CIT, Cap√≠tulo 15](https://inferentialthinking.com/)

Material adaptado do [DSC10 (UCSD)](https://dsc10.com/) por [Flavio Figueiredo (DCC-UFMG)](https://flaviovdf.io/fcd/) e [Uriel Silva (DEST-UFMG)](https://urielmoreirasilva.github.io)


```python
# Imports para esse t√≥pico.
import numpy as np
import babypandas as bpd
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
plt.style.use('ggplot')

# Op√ß√µes de como printar objetos do Numpy e do Pandas.
np.set_printoptions(threshold = 20, precision = 2, suppress = True)
pd.set_option("display.max_rows", 7)
pd.set_option("display.max_columns", 8)
pd.set_option("display.precision", 2)
```


```python
## Fun√ß√µes que definimos no t√≥pico anterior.

# Padroniza√ß√£o das colunas de um DataFrame.
def standard_units(col):
    return (col - col.mean()) / np.std(col)

# C√°lculo do coeficiente de correla√ß√£o r.
def calculate_r(df, x, y):
    '''Returns the average value of the product of x and y, 
       when both are measured in standard units.'''
    x_su = standard_units(df.get(x))
    y_su = standard_units(df.get(y))
    return (x_su * y_su).mean()
```


```python
## Fun√ß√µes s√£o para visualiza√ß√£o de uma reta de regress√£o.

# Reta de regress√£o.
def plot_regression_line(df, x, y, margin=.02):
    '''Computes the slope and intercept of the regression line between columns x and y in df (in original units) and plots it.'''
    m = slope(df, x, y)
    b = intercept(df, x, y)
    
    df.plot(kind='scatter', x=x, y=y, s=100, figsize=(10, 5), label='original data')
    left = df.get(x).min()*(1 - margin)
    right = df.get(x).max()*(1 + margin)
    domain = np.linspace(left, right, 10)
    plt.plot(domain, m*domain + b, color='orange', label='regression line', lw=4)
    plt.suptitle(format_equation(m, b), fontsize=18)
    plt.legend();

# Adicionar equa√ß√£o √† reta de regress√£o,
def format_equation(m, b):
    if b > 0:
        return r'$y = %.2fx + %.2f$' % (m, b)
    elif b == 0:
        return r'$y = %.2fx' % m
    else:
        return r'$y = %.2fx %.2f$' % (m, b)

# Gr√°fico de dispers√£o dos erros.
def plot_errors(df, m, b, ax=None):
    x = df.get('x')
    y = m * x + b
    df.plot(kind='scatter', x='x', y='y', s=100, label='original data', ax=ax, figsize=(10, 5) if ax is None else None)
    
    if ax:
        plotter = ax
    else:
        plotter = plt
    
    plotter.plot(x, y, color='orange', lw=4)
    
    for k in np.arange(df.shape[0]):
        xk = df.get('x').iloc[k]
        yk = np.asarray(y)[k]
        if k == df.shape[0] - 1:
            plotter.plot([xk, xk], [yk, df.get('y').iloc[k]], linestyle=(0, (1, 1)), c='r', lw=4, label='errors')
        else:
            plotter.plot([xk, xk], [yk, df.get('y').iloc[k]], linestyle=(0, (1, 1)), c='r', lw=4)
    
    plt.title(format_equation(m, b), fontsize=18)
    # plt.xlim(50, 90)
    # plt.ylim(40, 100)
    plt.legend();
```

## Regress√£o Linear: Conceitos B√°sicos

Antes de apresentar formalmente os principais conceitos e propriedades do modelo de regress√£o linear, vamos considerar um exemplo motivador cl√°ssico da √°rea. 

### Exemplo: Predizendo alturas  üë™ üìè

Os dados desse exemplo consistem em um conjunto de medidas antropom√©tricas de v√°rias fam√≠lias, coletados no final do s√©culo XVIII por [Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton).
- Galton foi um dos pioneiros da Eugenia, e essa √© uma das principais raz√µes pelas quais ele coletou esses dados.
- A an√°lise sistem√°tica desses dados fez com que Galton recebesse reconhecimento como o descobridor do fen√¥meno de **regress√£o √† m√©dia** em certos fen√¥menos da natureza, e √† t√©cnica de regress√£o linear em geral.  


```python
galton = bpd.read_csv('data/galton.csv')
galton
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>family</th>
      <th>father</th>
      <th>mother</th>
      <th>midparentHeight</th>
      <th>children</th>
      <th>childNum</th>
      <th>gender</th>
      <th>childHeight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>78.5</td>
      <td>67.0</td>
      <td>75.43</td>
      <td>4</td>
      <td>1</td>
      <td>male</td>
      <td>73.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>78.5</td>
      <td>67.0</td>
      <td>75.43</td>
      <td>4</td>
      <td>2</td>
      <td>female</td>
      <td>69.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>78.5</td>
      <td>67.0</td>
      <td>75.43</td>
      <td>4</td>
      <td>3</td>
      <td>female</td>
      <td>69.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>931</th>
      <td>203</td>
      <td>62.0</td>
      <td>66.0</td>
      <td>66.64</td>
      <td>3</td>
      <td>3</td>
      <td>female</td>
      <td>61.0</td>
    </tr>
    <tr>
      <th>932</th>
      <td>204</td>
      <td>62.5</td>
      <td>63.0</td>
      <td>65.27</td>
      <td>2</td>
      <td>1</td>
      <td>male</td>
      <td>66.5</td>
    </tr>
    <tr>
      <th>933</th>
      <td>204</td>
      <td>62.5</td>
      <td>63.0</td>
      <td>65.27</td>
      <td>2</td>
      <td>2</td>
      <td>female</td>
      <td>57.0</td>
    </tr>
  </tbody>
</table>
<p>934 rows √ó 8 columns</p>
</div>



### Predizendo a altura de um filho (adulto) com base na altura de sua m√£e

Nesse exemplo, vamos focar na associa√ß√£o entre a altura de um filho (na fase adulta) e a altura de sua m√£e.


```python
male_children = galton[galton.get('gender') == 'male'].reset_index()
mom_son = bpd.DataFrame().assign(mom=male_children.get('mother'), son=male_children.get('childHeight'))
mom_son
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mom</th>
      <th>son</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>67.0</td>
      <td>73.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>66.5</td>
      <td>73.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>66.5</td>
      <td>72.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>478</th>
      <td>60.0</td>
      <td>66.0</td>
    </tr>
    <tr>
      <th>479</th>
      <td>66.0</td>
      <td>64.0</td>
    </tr>
    <tr>
      <th>480</th>
      <td>63.0</td>
      <td>66.5</td>
    </tr>
  </tbody>
</table>
<p>481 rows √ó 2 columns</p>
</div>




```python
mom_son.plot(kind='scatter', x='mom', y='son');
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_12_0.png)
    


O diagrama de dispers√£o parece demonstrar uma associa√ß√£o linear e positiva entre a altura dos filhos e a altura das m√£es, ainda que um pouco fraca.

Esperamos ent√£o que $r > 0$, e que $|r|$ n√£o seja muito pr√≥ximo de 1.


```python
r_mom_son = calculate_r(mom_son, 'mom', 'son')
r_mom_son
```




    0.3230049836849053



### Diferentes maneiras de fazer uma previs√£o

- Denote por $Y_i$ a altura do $i$-√©simo filho e por $X_i$ a altura da m√£e correspondente, $i = 1, \ldots, n$.
    - Em um contexto de regress√£o, denominamos $Y_i$ de **vari√°vel dependente**, ou **vari√°vel explicada**.
    - Analogamente, denominamos $X_i$ de **preditor** ou **vari√°vel explicativa**. 

- Comecemos estabelecendo a estrat√©gia mais simples poss√≠vel para prever a altura de um filho.
    - Podemos ent√£o tomar, por exemplo, simplesmente a _m√©dia_ das alturas de todos os filhos, independente das alturas de suas m√£es.
    - Em um contexto de regress√£o, denotamos **o valor predito** de $Y_i$ como $\hat{Y}_i$.
    - Nesse caso temos ent√£o $\hat{Y}_i = \bar{Y}$ para todo $i$.


```python
y_bar = mom_son.get('son').mean()
y_bar
```




    69.23409563409565



A m√©dia das alturas dos filhos √© $\bar{Y} \simeq 69.23$.

Dessa forma, visualmente, temos:


```python
mom_son.plot(kind='scatter', x='mom', y='son', title='Previs√£o para Y: m√©dia das alturas dos filhos', figsize=(10, 5));
plt.axhline(y_bar, color='orange', lw=4);
#plt.xlim(-3, 3);
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_21_0.png)
    


- Para simplificar mais ainda nosso problema, podemos _padronizar_ $Y_i$ e $X_i$.
    - Dessa forma, como a m√©dia de $Y_i$ em unidades padr√£o √© sempre igual a 0, isto √©, $\bar{Y}_{i \: (\text{su})} = 0$, os valores preditos aqui s√£o simplesmente $\hat{Y}_{i \: (\text{su})} = 0$.

Visualmente, no diagrama de dispers√£o das vari√°veis padronizadas, temos:


```python
def standardize(df):
    """Return a DataFrame in which all columns of df are converted to standard units."""
    df_su = bpd.DataFrame()
    for column in df.columns:
        # This uses syntax that is out of scope; don't worry about how it works.
        df_su = df_su.assign(**{column + ' (su)': standard_units(df.get(column))})
    return df_su
```


```python
mom_son_su = standardize(mom_son)

mom_son_su.plot(kind='scatter', x='mom (su)', y='son (su)', title='Previs√£o para Y (padronizada): m√©dia das alturas (padronizadas) dos filhos', figsize=(10, 5));
plt.axhline(0, color='orange', lw=4);
plt.xlim(-3, 3);
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_25_0.png)
    


### Melhorando nossas previs√µes

- Podemos tentar utilizar o fato de que _existe uma associa√ß√£o linear_ (lembre que $r \simeq 0.32$) entre a altura dos filhos $Y_i$ e a altura das m√£es $X_i$ para _melhorar nossas previs√µes_ sobre $Y_i$.

- Formalmente, isso equivale a tomar $\hat{Y}_i := b + a \cdot X_i$ para algum $a$ e $b$ reais.

- Agora, isso naturalmente leva √† seguinte pergunta: dado que existem infinitas retas do tipo $Y_i = b + a \cdot X_i$, qual delas seria a "melhor"? ü§î

- Veremos em outros cursos que o conceito de "melhor" pode mudar de situa√ß√£o para situa√ß√£o, mas a seguinte caracteriza√ß√£o √© muito √∫til na pr√°tica:

> A **melhor** reta (e logo denominada de **reta de regress√£o**) √© aquela que **minimiza a dist√¢ncia entre os valores preditos $\hat{Y}_i$ e os valores observados $Y_i$**.

- Naturalmente, existem tamb√©m diferentes medidas de "dist√¢ncia" que podem ser utilizadas, mas aqui nos limitaremos √† **dist√¢ncia Euclidiana**.

- Como a dist√¢ncia Euclidiana entre dois vetores $\mathbf{x} := (x_1, \ldots, x_n)$ e $\mathbf{y} := (y_1, \ldots, y_n)$ √© dada por

\begin{equation*}
    D(\mathbf{x}, \mathbf{y}) := \sqrt{\sum^n_{i=1} (x_i - y_i)^2},
\end{equation*}

ent√£o a dist√¢ncia entre $\hat{\mathbf{Y}} := (\hat{Y}_1, \ldots, \hat{Y}_n)$ e $\mathbf{Y} := (Y_1, \ldots, Y_n)$ √© dada por

\begin{equation*}
    D(\hat{\mathbf{Y}}_i, \mathbf{Y}_i) := \sqrt{\sum^n_{i=1} (\hat{Y}_i - Y_i)^2}.
\end{equation*}

<u>Nota t√©cnica</u>: Apesar de tomar a "diferen√ßa simples" entre $\hat{\mathbf{Y}}$ e $\mathbf{Y}$, isto √©, $\sum^n_{i=1} (\hat{Y}_i - Y_i)$ ser mais natural do que pensar em termos de dist√¢ncia Euclidiana, na pr√°tica isso n√£o funciona muito bem.

Se lembrarmos da nossa discuss√£o sobre desvio padr√£o e vari√¢ncia, √© poss√≠vel mostrar que **qualquer reta** $\hat{Y}_i = b + a \cdot X_i$ satisfaz $\sum^n_{i=1} (\hat{Y}_i - Y_i) = 0$, n√£o s√≥ a reta de regress√£o.

- Mostraremos mais adiante como encontramos $a$ e $b$ para um problema de regress√£o em geral, mas **no caso padronizado a reta de regress√£o √© simplesmente uma reta com intercepto $b = 0$ e inclina√ß√£o $a = r$**.

Visualmente,


```python
x = np.linspace(-3, 3)
y = x * r_mom_son
mom_son_su.plot(kind='scatter', x='mom (su)', y='son (su)', title=r'Reta de regress√£o (Y e X padronizados): intercepto = 0 e inclina√ß√£o = $r$', figsize=(10, 5));
plt.plot(x, y, color='orange', label='regression line', lw=4)
plt.xlim(-3, 3)
plt.legend();
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_36_0.png)
    


- Embora seja um pouco dif√≠cil de fazer esse julgamento com base apenas na evid√™ncia visual, a reta de regress√£o realmente minimiza a dist√¢ncia entre cada um de seus pontos $\hat{Y}_i = b + a \cdot X_i$ e os pontos $Y_i$ no diagrama de dispers√£o.

## Regress√£o Linear: Fazendo previs√µes

### A reta de regress√£o no caso padronizado

- Quando $Y_i$ e $X_i$ est√£o expressas em unidades padronizadas, temos $b = 0$ e $a = r$.

- Dessa forma, a reta de regress√£o no caso padronizado √© dada por

\begin{equation*}
    \hat{Y}_{i \: (\text{su})} := r \cdot X_{i \: (\text{su})}
\end{equation*}

- Como no nosso exemplo $r = 0.32$, isso significa por exemplo que:
    - se $X_{i \: (\text{su})} = 1$, ent√£o $\hat{Y}_{i \: (\text{su})} = 1 \cdot 0.32 = 0.32$;
    - se $X_{i \: (\text{su})} = 2$, ent√£o $\hat{Y}_{i \: (\text{su})} = 2 \cdot 0.32 = 0.64$;
    - se $X_{i \: (\text{su})} = -1$, ent√£o $\hat{Y}_{i \: (\text{su})} = -1 \cdot 0.32 = -0.32$;
    - se $X_{i \: (\text{su})} = -0.5$, ent√£o $\hat{Y}_{i \: (\text{su})} = -0.5 \cdot 0.32 = -0.16$
 
e assim em diante.

- Lembrando que em unidades padr√£o cada valor $X_{i \: (\text{su})} = x$ significa "$x$ desvios padr√£o da m√©dia", ent√£o para cada $x = 1$ desvio acima (ou abaixo) da m√©dia que a altura da m√£e est√°, a altura do seu filho _predita pela regress√£o_ est√° a apenas 0.32 desvios acima (ou abaixo) da m√©dia.
    - Note que tanto para as m√£es $X_i$ quanto para os filhos $Y_i$, esses desvios s√£o _com rela√ß√£o √†s suas pr√≥prias m√©dias_, isto √© $\bar{X}$ e $\bar{Y}$.

- A reta de regress√£o sempre _prediz_ que um filho ter√° uma altura **mais pr√≥xima da m√©dia** do que sua m√£e.

- Esse efeito √© denominado de **regress√£o √† m√©dia**, e √© da√≠ que vem o termo "regress√£o".

- Note que n√£o necessariamente _todo_ filho vai ter uma altura mais pr√≥xima da m√©dia do que sua m√£e.
    - A reta de regress√£o prev√™ apenas a _m√©dia_ de $Y_i$ dado um certo valor de $X_i$, e logo esse processo sempre envolve _erros_ (para mais ou para menos).

Vamos elaborar mais sobre os erros de uma regress√£o e suas propriedades abaixo.

### A reta de regress√£o no caso geral

- Apesar da reta de regress√£o obtida anteriormente com unidades padronizadas ser simples de obter e de interpretar, as predi√ß√µes s√£o dadas em desvios em torno da m√©dia de $Y_i$ com rela√ß√£o aos desvios em torno da m√©dia de $X_i$.

- Isso √© um pouco incoveniente, porque se quisermos encontrar o valor predito de $\hat{Y}_i$ ao inv√©s de $\hat{Y}_{i \: (\text{su})}$, ent√£o teremos que:
    1. Encontrar o valor de $X_i$ correspondente a $X_{i \: (\text{su})}$ utilizando a rela√ß√£o $X_i = \bar{X} + S_x \cdot X_{i \: (\text{su})}$;
    2. Prever o valor de $Y_{i \: (\text{su})}$ atrav√©s de $\hat{Y}_{i \: (\text{su})} = r \cdot X_{i \: (\text{su})}$;
    3. Encontrar o valor de $Y_i$ correspondente a $Y_{i \: (\text{su})}$ utilizando a rela√ß√£o $Y_i = \bar{Y} + S_y \cdot \hat{Y}_{i \: (\text{su})}$.

<u>Nota t√©cnica</u>: No processo acima, utilizamos o fato de que $\bar{\hat{Y}}_i = \bar{Y}$. Isso pode ser demonstrado analiticamente, mas a seguinte figura pode ajudar muito na intui√ß√£o geom√©trica desse processo:

<center><img src="data/original_standard.png" width=50%></center>

- Fazendo um pouco de √°lgebra, utilizando as rela√ß√µes acima podemos reescrever a equa√ß√£o da reta de regress√£o do caso padronizado, i.e. dada por

\begin{equation*}
    \hat{Y}_{i \: (\text{su})} = r \cdot X_{i \: (\text{su})} \:\:\: \Leftrightarrow \:\:\: \frac{\hat{Y}_i - \bar{Y}}{S_y} = r \cdot \frac{X_i - \bar{X}}{S_x}
\end{equation*}

como

\begin{equation*}
    \hat{Y}_i = \left(\bar{Y} - r \cdot \frac{S_y}{S_x} \bar{X}\right) + r \cdot \frac{S_y}{S_x} X_i
\end{equation*}

Isto √©, a reta de regress√£o no caso geral √© uma reta do tipo $\hat{Y}_i = b + a \cdot X_i$, onde o **intercepto da regress√£o** $b$ e o **coeficiente de inclina√ß√£o da regress√£o** $a$ s√£o dados por

\begin{align*}
    b &= \bar{Y} - a \cdot \bar{X}, & a &= r \cdot \frac{S_y}{S_x}
\end{align*}

√â poss√≠vel obter a reta de regress√£o para o caso padronizado diretamente da f√≥rmula acima para o caso geral, uma vez que, se $X_i$ e $Y_i$ s√£o padronizadas, ent√£o temos $\bar{X} = \bar{Y} = 0$ e $S_x = S_y = 1$.

Vamos implementar essas f√≥rmulas no Python e calcular $a$ e $b$ no caso geral.


```python
def slope(df, x, y):
    "Returns the slope of the regression line between columns x and y in df (in original units)."
    r = calculate_r(df, x, y)
    return r * np.std(df.get(y)) / np.std(df.get(x))

def intercept(df, x, y):
    "Returns the intercept of the regression line between columns x and y in df (in original units)."
    return df.get(y).mean() - slope(df, x, y) * df.get(x).mean()
```


```python
a_heights = slope(mom_son, 'mom', 'son')
a_heights
```




    0.3650611602425757




```python
b_heights = intercept(mom_son, 'mom', 'son')
b_heights
```




    45.8580379719931



Dessa forma, nesse exemplo a reta de regress√£o √© dada por

$$\hat{Y}_i = 45.858 + 0.365 \cdot X_i$$


```python
def predict_son(mom):
    return a_heights * mom + b_heights
```


```python
xs = np.arange(57, 72)
ys = predict_son(xs)
mom_son.plot(kind='scatter', x='mom', y='son', figsize=(10, 5), title='Reta de regress√£o (geral): intercepto = 45.858 e inclina√ß√£o = $0.365$', label='original data');
plt.plot(xs, ys, color='orange', lw=4, label='regression line')
plt.legend();
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_60_0.png)
    


- Um fato importante sobre a reta de regress√£o no caso geral (e crucial para a interpretabilidade dos resultados) √© que **os valores preditos $\hat{Y}_i$ sempre s√£o medidos nas mesmas unidades de $Y_i$**.
    - Para estabelecer isso, basta ver que $r$ n√£o tem unidade de medida, $\bar{X}$ e $S_x$ t√™m as mesmas unidades de medida de $X_i$ e $\bar{Y}$ e $S_y$ t√™m as mesmas unidades de medida de $Y_i$.
 
> Em outras palavras, a regress√£o mant√©m a mesma associa√ß√£o linear entre $Y_i$ e $X_i$ apenas _ajustando a inclina√ß√£o correspondente_, qualquer que seja a unidade de medida de $X_i$.

No nosso exemplo em quest√£o, ainda que $X_i$ n√£o estivesse expresso em polegadas, $\hat{Y}_i$ estaria expresso em polegadas devido _apenas_ ao fato de $Y_i$ estar expresso em polegadas!

- Uma √∫ltima nota acerca da regress√£o linear √© que a reta de regress√£o define um **modelo estat√≠stico** para a rela√ß√£o entre $X_i$ e $Y_i$.
    - Mais especificamente, o **modelo de regress√£o linear** diz que _a rela√ß√£o m√©dia_ entre $Y_i$ e $X_i$ √© dada por uma reta do tipo $\hat{Y}_i = b + a \cdot X_i$.
    - Voc√™ ver√° uma formaliza√ß√£o completa do modelo de regress√£o linear em outros cursos, mas por enquanto √© importante entender que esse √© um modelo bem definido, e que **$a$ e $b$ s√£o simplesmente par√¢metros populacionais a serem estimados** atrav√©s de estat√≠sticas.

### Fazendo previs√µes com a reta de regress√£o no caso geral

Qual √© a altura predita de um filho cuja m√£e tem 62 polegadas de altura?


```python
predict_son(62)
```




    68.4918299070328




```python
## In centimeters!
print(62*2.54)
predict_son(62)*2.54
```

    157.48
    




    173.9692479638633



E se a m√£e tiver 55 polegadas de altura? E 73?


```python
predict_son(55)
```




    65.93640178533477




```python
## In centimeters!
print(55*2.54)
predict_son(55)
```

    139.7
    




    65.93640178533477




```python
predict_son(73)
```




    72.50750266970113




```python
## In centimeters!
print(73*2.54)
predict_son(73)*2.54
```

    185.42000000000002
    




    184.16905678104087



### Exerc√≠cio ‚úÖ

Considere um curso em que as notas de uma avalia√ß√£o parcial $X_i$ relativamente simples apresentam m√©dia $\bar{X} = 80$ e desvio padr√£o $S_x = 15$, e em que as notas de uma avalia√ß√£o final $Y_i$ relativamente complexa apresentam m√©dia $\bar{Y} = 50$ e desvio padr√£o $S_y = 12$. 

Se o diagrama de dispers√£o entre $X_i$ e $Y_i$ mostra uma rela√ß√£o linear razo√°vel e a correla√ß√£o entre $X_i$ e $Y_i$ √© igual a $r = 0.75$, qual √© o valor predito $\hat{Y}_i$ por uma regress√£o da nota final de um estudante que recebeu um $X_i = 90$ na avalia√ß√£o parcial?

- A. 54
- B. 56
- C. 58
- D. 60
- E. 62

## Outliers

### O efeito dos outliers na correla√ß√£o

Considere o seguinte conjunto de dados:


```python
outlier = bpd.read_csv('data/outlier.csv')
outlier.plot(kind='scatter', x='x', y='y', s=100, figsize=(10, 5));
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_77_0.png)
    


Quanto voc√™ diria ser a correla√ß√£o entre $x$ e $y$?


```python
calculate_r(outlier, 'x', 'y')
```




    -0.02793982443854448




```python
plot_regression_line(outlier, 'x', 'y')
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_80_0.png)
    


A maior parte dos dados parece apresentar uma _rela√ß√£o linear positiva_ entre $x$ e $y$, mas por alguma raz√£o $r < 0$ e a reta de regress√£o apresentou uma inclina√ß√£o negativa.

E se remov√©ssemos o outlier? ü§î


```python
without_outlier = outlier[outlier.get('y') > 40]
```


```python
calculate_r(without_outlier, 'x', 'y')
```




    0.9851437295364018




```python
plot_regression_line(without_outlier, 'x', 'y')
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_85_0.png)
    


Agora o resultado parece correto! üëç

- **Importante**: Em muitas situa√ß√µes, um _√∫nico_ outlier pode ter um impacto expressivo na correla√ß√£o e, logo, na reta de regress√£o.
    - A sensibilidade (ou falta de robustez) da correla√ß√£o √† valores discrepantes √© "herdada" da m√©dia, uma vez que a correla√ß√£o √© fun√ß√£o das m√©dias e desvios padr√£o de $X_i$ e $Y_i$.
    - O mesmo vale para o coeficiente de inclina√ß√£o em uma reta de regress√£o.

- Quando realizamos uma an√°lise de associa√ß√£o e/ou regress√£o, √© sempre importante verificar se um ou mais pontos no conjunto de dados em quest√£o s√£o outliers.
    - Embora existam crit√©rios objetivos para fazer essa classifica√ß√£o, uma an√°lise visual preliminar √© sempre importante (e muitas vezes suficiente).

## Erros de previs√£o

### Motiva√ß√£o

- Nos exemplos que vimos at√© agora, a reta de regress√£o parece se ajustar "bem" aos nosso dados.
    - Mas **qu√£o bem**?
    - O que faz com que uma reta de regress√£o seja "boa"?
    - E em qual sentido a reta de regress√£o √© a "melhor"?

- Para medir a **qualidade do ajuste** de um modelo de regress√£o aos nossos dados, definimos o **erro de previs√£o** (tamb√©m conhecido como **res√≠duo** da regress√£o) para a $i$-√©sima observa√ß√£o como

$$\hat{\epsilon}_i := Y_i - \hat{Y}_i.$$

Isto √©, cada $\hat{\epsilon}_i$ representa o _qu√£o bem_ o modelo (atrav√©s de $\hat{Y}_i$) consegue prever as observa√ß√µes $Y_i$.

### Examplo: Erros de previs√£o com e sem outliers

Voltando ao exemplo anterior, considere como os erros de previs√£o ficariam com o outlier presente no banco de dados:


```python
a_outlier = slope(outlier, 'x', 'y')
b_outlier = intercept(outlier, 'x', 'y')

a_outlier, b_outlier
```




    (-0.046859231138684336, 67.81884376997135)




```python
plot_errors(outlier, a_outlier, b_outlier)
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_96_0.png)
    


Naturalmente, os erros de previs√£o s√£o bem expressivos, principalmente para o outlier.

Ap√≥s remover o outlier, os resultados s√£o:


```python
m_no_outlier = slope(without_outlier, 'x', 'y')
b_no_outlier = intercept(without_outlier, 'x', 'y')

m_no_outlier, b_no_outlier
```




    (0.9759277157245881, 3.042337135297416)




```python
plot_errors(without_outlier, m_no_outlier, b_no_outlier)
```


    
![png](18%20%E2%80%93%20Regressao_files/18%20%E2%80%93%20Regressao_100_0.png)
    


Bem melhor! üëç

## Resumo

- A reta de regress√£o de $Y$ em $X$ √© dada por
$$\hat{Y}_i = b + a \cdot X_i,$$
em que
\begin{align*}
    b &= \bar{Y} - a \cdot \bar{X}, & a &= r \cdot \frac{S_y}{S_x}.
\end{align*}

- Essa √© a reta que melhor se ajusta √† uma rela√ß√£o linear entre $Y_i$ e $X_i$, **minimizando a dist√¢ncia entre os valores observados $Y_i$ e os valores preditos $\hat{Y}_i$**.

- Quando estimamos uma reta de regress√£o, devemos sempre tomar cuidado com os **outliers**!
